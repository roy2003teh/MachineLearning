{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3723046146.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    https://aclanthology.org/2020.acl-main.90.pdf\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "https://aclanthology.org/2020.acl-main.90.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PySimpleGUI in c:\\programdata\\anaconda3\\lib\\site-packages (5.0.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: rsa in c:\\programdata\\anaconda3\\lib\\site-packages (from PySimpleGUI) (4.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from rsa->PySimpleGUI) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "pip install PySimpleGUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch torchvision torchaudio cudatoolkit=xx.x -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\tarumt\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\tarumt\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\tarumt\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tarumt\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@waqarhaider1292/chatbot-by-huggingface-document-based-question-answering-c4fd94ec35db\n",
    "\n",
    "https://medium.com/@lokaregns/question-answering-with-hugging-face-transformers-a-beginners-guide-487ae1a91b9a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # Load the question answering pipeline\n",
    "# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "# qa_pipeline = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "# # Initialize conversation history dictionary\n",
    "# conversation_history = {\n",
    "#     \"context\": None,\n",
    "#     \"questions\": [],\n",
    "#     \"answers\": []\n",
    "# }\n",
    "\n",
    "# def answer_question(context, question):\n",
    "#     # Use the QA pipeline to answer the question based on the context\n",
    "#     try:\n",
    "#         result = qa_pipeline(question=question, context=context)\n",
    "#         return result\n",
    "#     except Exception as e:\n",
    "#         print(\"Error occurred while answering the question:\", e)\n",
    "#         return \"Sorry, I couldn't find an answer to that question.\"\n",
    "\n",
    "# def detect_follow_up(question):\n",
    "#     # Simple algorithm to detect follow-up questions\n",
    "#     keywords = [\"next\", \"then\", \"after\", \"following\"]\n",
    "#     for keyword in keywords:\n",
    "#         if keyword in question.lower():\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# def add_to_conversation_history(context, question, answer):\n",
    "#     # Add the current context, question, and answer to conversation history\n",
    "#     conversation_history[\"context\"] = context\n",
    "#     conversation_history[\"questions\"].append(question)\n",
    "#     conversation_history[\"answers\"].append(answer)\n",
    "\n",
    "# def answer_with_history(question):\n",
    "#     # Check if the current question is a follow-up question\n",
    "#     is_follow_up = detect_follow_up(question)\n",
    "    \n",
    "#     if is_follow_up:\n",
    "#         # If it's a follow-up question, use the context and previous answers to answer\n",
    "#         context = conversation_history[\"context\"]\n",
    "#         questions = conversation_history[\"questions\"]\n",
    "#         answers = conversation_history[\"answers\"]\n",
    "#         for i, prev_question in enumerate(reversed(questions)):\n",
    "#             if detect_follow_up(prev_question):\n",
    "#                 continue\n",
    "#             context = context.replace(answers[-(i+1)], \"\")  # Remove previous answer from context\n",
    "#             break\n",
    "#         return answer_question(context, question)\n",
    "#     else:\n",
    "#         # If it's not a follow-up question, answer it directly\n",
    "#         return answer_question(conversation_history[\"context\"], question)\n",
    "\n",
    "# # Example context\n",
    "# context = \"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during the Eisenhower administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal for the 1960s of 'landing a man on the Moon and returning him safely to the Earth'.\"\n",
    "\n",
    "# # Example questions\n",
    "# questions = [\n",
    "#     \"What was the Apollo program?\",\n",
    "#     \"after Who carried out the Apollo program?\",\n",
    "#     \"When did the Apollo program take place?\",\n",
    "#     \"What happened after the Apollo program?\",\n",
    "#     \"How was the Apollo program followed?\"\n",
    "# ]\n",
    "\n",
    "# # Answer the questions with conversation history tracking\n",
    "# for question in questions:\n",
    "#     answer = answer_with_history(question)\n",
    "#     add_to_conversation_history(context, question, answer)\n",
    "#     print(\"Question:\", question)\n",
    "#     print(\"Answer:\", answer)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import PySimpleGUI as sg\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the question answering pipeline\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "# Initialize conversation history dictionary\n",
    "conversation_history = {\n",
    "    \"context\": None,\n",
    "    \"questions\": [],\n",
    "    \"answers\": []\n",
    "}\n",
    "\n",
    "def answer_question(context, question):\n",
    "    try:\n",
    "        if len(conversation_history['questions']) != 0:\n",
    "            concatenated_text = \"\"\n",
    "            for history_question, answer in zip(conversation_history[\"questions\"], conversation_history[\"answers\"]):\n",
    "                concatenated_text += f\"{history_question} {answer}. \"\n",
    "            \n",
    "            question = concatenated_text + question\n",
    "\n",
    "\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        return question, result['answer'], result['score']\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while answering the question:\", e)\n",
    "        return \"Sorry, I couldn't find an answer to that question.\"\n",
    "\n",
    "\n",
    "\n",
    "def add_to_conversation_history(context, question, answer):\n",
    "    # Add the current context, question, and answer to conversation history\n",
    "    conversation_history[\"context\"] = context\n",
    "    conversation_history[\"questions\"].append(question)\n",
    "    conversation_history[\"answers\"].append(answer)\n",
    "\n",
    "# def answer_with_history(question):\n",
    "\n",
    "#         # If it's a follow-up question, use the context and previous answers to answer\n",
    "#         context = conversation_history[\"context\"]\n",
    "#         questions = conversation_history[\"questions\"]\n",
    "#         answers = conversation_history[\"answers\"]\n",
    "#         return answer_question(context, question)\n",
    "#         # If it's not a follow-up question, answer it directly\n",
    "\n",
    "# Define GUI layout\n",
    "layout = [\n",
    "    [sg.Text(\"Question-Answering System\")],\n",
    "    [sg.Text(\"Context:\"), sg.Multiline(key='-CONTEXT-', size=(60, 10))],\n",
    "    [sg.Multiline(size=(60, 10), key='-OUTPUT-', disabled=True)],\n",
    "    [sg.InputText(key='-INPUT-')],\n",
    "    [sg.Button('Ask'), sg.Button('Exit')],\n",
    "]\n",
    "\n",
    "# Create the GUI window\n",
    "window = sg.Window('QA System', layout)\n",
    "\n",
    "# Main event loop\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "\n",
    "    if event == sg.WINDOW_CLOSED or event == 'Exit':\n",
    "        break\n",
    "\n",
    "    user_input = values['-INPUT-']\n",
    "    context = values['-CONTEXT-']\n",
    "\n",
    "    if user_input:\n",
    "        question = user_input\n",
    "\n",
    "        # Get response to the user's question\n",
    "        concat, answer, score = answer_question(context, question)\n",
    "        add_to_conversation_history(context, question, answer)\n",
    "\n",
    "\n",
    "        # Display the answer in the GUI window\n",
    "        window['-OUTPUT-'].print(\"Question: \" + concat)\n",
    "        window['-OUTPUT-'].print(\"Answer: \" + answer)\n",
    "        window['-OUTPUT-'].print(\"Score: \" + str(score))\n",
    "        window['-OUTPUT-'].print(\"\")\n",
    "\n",
    "window.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load English language model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Define two example sentences\u001b[39;00m\n\u001b[0;32m      7\u001b[0m sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho is b? Tong.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define two example sentences\n",
    "sentence1 = \"who is b? Tong.\"\n",
    "sentence2 = \"cat\"\n",
    "\n",
    "# Process the sentences with spaCy\n",
    "doc1 = nlp(sentence1)\n",
    "doc2 = nlp(sentence2)\n",
    "\n",
    "# Calculate cosine similarity between the document vectors\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# from datasets import load_metric\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# bleu = load_metric(\"bleu\")\n",
    "\n",
    "# # Reference answer\n",
    "# reference = \"The cat is on the mat.\"\n",
    "\n",
    "# # Generated answer\n",
    "# generated = \"There is a cat on the mat.\"\n",
    "\n",
    "# # Tokenize reference and generated answers\n",
    "# reference_tokens = word_tokenize(reference.lower())\n",
    "# generated_tokens = word_tokenize(generated.lower())\n",
    "\n",
    "# # Calculate BLEU score\n",
    "# bleu_score = bleu.compute(predictions=[reference_tokens], references=[generated_tokens])\n",
    "# print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Obtaining dependency information for PyPDF2 from https://files.pythonhosted.org/packages/8e/5e/c86a5643653825d3c913719e788e41386bee415c2b87b4f955432f2de6b2/pypdf2-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "   -------------------------------------- - 225.3/232.6 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 232.6/232.6 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PySimpleGUI as sg\n",
    "# from transformers import pipeline\n",
    "# import PyPDF2\n",
    "\n",
    "# # Load the question answering pipeline\n",
    "# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "# qa_pipeline = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "# # Initialize conversation history dictionary\n",
    "# conversation_history = {\n",
    "#     \"context\": None,\n",
    "#     \"questions\": [],\n",
    "#     \"answers\": []\n",
    "# }\n",
    "\n",
    "# def answer_question(context, question):\n",
    "#     try:\n",
    "#         if len(conversation_history['questions']) != 0:\n",
    "#             concatenated_text = \"\"\n",
    "#             for history_question, answer in zip(conversation_history[\"questions\"], conversation_history[\"answers\"]):\n",
    "#                 concatenated_text += f\"{history_question} {answer}. \"\n",
    "            \n",
    "#             question = concatenated_text + question\n",
    "\n",
    "\n",
    "#         result = qa_pipeline(question=question, context=context)\n",
    "#         return question, result['answer'], result['score']\n",
    "#     except Exception as e:\n",
    "#         print(\"Error occurred while answering the question:\", e)\n",
    "#         return \"Sorry, I couldn't find an answer to that question.\"\n",
    "\n",
    "# def add_to_conversation_history(context, question, answer):\n",
    "#     # Add the current context, question, and answer to conversation history\n",
    "#     conversation_history[\"context\"] = context\n",
    "#     conversation_history[\"questions\"].append(question)\n",
    "#     conversation_history[\"answers\"].append(answer)\n",
    "\n",
    "# def extract_text_from_pdf(pdf_file_path):\n",
    "#     text = \"\"\n",
    "#     with open(pdf_file_path, 'rb') as file:\n",
    "#         pdf_reader = PyPDF2.PdfFileReader(file)\n",
    "#         for page_num in range(pdf_reader.numPages):\n",
    "#             page = pdf_reader.getPage(page_num)\n",
    "#             text += page.extractText()\n",
    "#     return text\n",
    "\n",
    "# # Define GUI layout\n",
    "# layout = [\n",
    "#     [sg.Text(\"Question-Answering System\")],\n",
    "#     [sg.Text(\"PDF File:\"), sg.InputText(key='-PDF_FILE-'), sg.FileBrowse()],\n",
    "#     [sg.Text(\"Context:\"), sg.Multiline(key='-CONTEXT-', size=(60, 10))],\n",
    "#     [sg.Multiline(size=(60, 10), key='-OUTPUT-', disabled=True)],\n",
    "#     [sg.InputText(key='-INPUT-')],\n",
    "#     [sg.Button('Load PDF'), sg.Button('Ask'), sg.Button('Exit')],\n",
    "# ]\n",
    "\n",
    "# # Create the GUI window\n",
    "# window = sg.Window('QA System', layout)\n",
    "\n",
    "# # Main event loop\n",
    "# while True:\n",
    "#     event, values = window.read()\n",
    "\n",
    "#     if event == sg.WINDOW_CLOSED or event == 'Exit':\n",
    "#         break\n",
    "\n",
    "#     if event == 'Load PDF':\n",
    "#         pdf_file_path = values['-PDF_FILE-']\n",
    "#         if pdf_file_path:\n",
    "#             context = extract_text_from_pdf(pdf_file_path)\n",
    "#             window['-CONTEXT-'].update(context)\n",
    "\n",
    "#     if event == 'Ask':\n",
    "#         user_input = values['-INPUT-']\n",
    "#         context = values['-CONTEXT-']\n",
    "\n",
    "#         if user_input:\n",
    "#             question = user_input\n",
    "\n",
    "#             # Get response to the user's question\n",
    "#             concat, answer, score = answer_question(context, question)\n",
    "#             add_to_conversation_history(context, question, answer)\n",
    "\n",
    "#             # Display the answer in the GUI window\n",
    "#             window['-OUTPUT-'].print(\"Question: \" + concat)\n",
    "#             window['-OUTPUT-'].print(\"Answer: \" + answer)\n",
    "#             window['-OUTPUT-'].print(\"Score: \" + str(score))\n",
    "#             window['-OUTPUT-'].print(\"\")\n",
    "\n",
    "# window.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import PySimpleGUI as sg\n",
    "from transformers import pipeline\n",
    "import PyPDF2\n",
    "\n",
    "# Load the question answering pipeline\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "# Initialize conversation history dictionary\n",
    "conversation_history = {\n",
    "    \"context\": None,\n",
    "    \"questions\": [],\n",
    "    \"answers\": []\n",
    "}\n",
    "\n",
    "def answer_question(context, question):\n",
    "    try:\n",
    "        if len(conversation_history['questions']) != 0:\n",
    "            concatenated_text = \"\"\n",
    "            for history_question, answer in zip(conversation_history[\"questions\"], conversation_history[\"answers\"]):\n",
    "                concatenated_text += f\"{history_question} {answer}. \"\n",
    "            \n",
    "            question = concatenated_text + question\n",
    "\n",
    "\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        return question, result['answer'], result['score']\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while answering the question:\", e)\n",
    "        return \"Sorry, I couldn't find an answer to that question.\"\n",
    "\n",
    "def add_to_conversation_history(context, question, answer):\n",
    "    # Add the current context, question, and answer to conversation history\n",
    "    conversation_history[\"context\"] = context\n",
    "    conversation_history[\"questions\"].append(question)\n",
    "    conversation_history[\"answers\"].append(answer)\n",
    "\n",
    "def extract_text_from_pdf(pdf_file_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Define GUI layout\n",
    "layout = [\n",
    "    [sg.Text(\"Question-Answering System\")],\n",
    "    [sg.Text(\"PDF File:\"), sg.InputText(key='-PDF_FILE-'), sg.FileBrowse()],\n",
    "    [sg.Text(\"Context:\"), sg.Multiline(key='-CONTEXT-', size=(60, 10))],\n",
    "    [sg.Multiline(size=(60, 10), key='-OUTPUT-', disabled=True)],\n",
    "    [sg.InputText(key='-INPUT-')],\n",
    "    [sg.Button('Load PDF'), sg.Button('Ask'), sg.Button('Exit')],\n",
    "]\n",
    "\n",
    "# Create the GUI window\n",
    "window = sg.Window('QA System', layout)\n",
    "\n",
    "# Main event loop\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "\n",
    "    if event == sg.WINDOW_CLOSED or event == 'Exit':\n",
    "        break\n",
    "\n",
    "    if event == 'Load PDF':\n",
    "        pdf_file_path = values['-PDF_FILE-']\n",
    "        if pdf_file_path:\n",
    "            context = extract_text_from_pdf(pdf_file_path)\n",
    "            window['-CONTEXT-'].update(context)\n",
    "\n",
    "    if event == 'Ask':\n",
    "        user_input = values['-INPUT-']\n",
    "        context = values['-CONTEXT-']\n",
    "\n",
    "        if user_input:\n",
    "            question = user_input\n",
    "\n",
    "            # Get response to the user's question\n",
    "            concat, answer, score = answer_question(context, question)\n",
    "            add_to_conversation_history(context, question, answer)\n",
    "\n",
    "            # Display the answer in the GUI window\n",
    "            window['-OUTPUT-'].print(\"Question: \" + concat)\n",
    "            window['-OUTPUT-'].print(\"Answer: \" + answer)\n",
    "            window['-OUTPUT-'].print(\"Score: \" + str(score))\n",
    "            window['-OUTPUT-'].print(\"\")\n",
    "\n",
    "window.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
